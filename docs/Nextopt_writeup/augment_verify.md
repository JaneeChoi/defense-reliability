
The role of #augmenting #lifting #dual 
1. Explore augmenting techniques: Origin X $\xrightarrow{\text{Lift}\; f}$ Augmented (X, f(X))
- newly created structure as following on augmented space prompts 2 i.e. efficient update.
	- Convex structure through dual variable: Lagrangian-related 
	- Vector flow structure through dual variable: HMC, Augmented neural ODE(?)
	- Markov structure through extra state: CVaR MDP, Stopping time
	- Sampling structure through latent var: EM, Imputation
	- Efficiency and diversity through copy: Coupling, Splitting


| algorithm               | Original, <br> Aug (=Ori., New), New        | New                          | Effect                                                 | Constrain/Invariant/Conserve                                                                                                      | Ref                                                                                                                                                                                                                   |
| ----------------------- | ------------------------------------------- | ---------------------------- | ------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| SBC                     | $\theta, (\theta, y)$                       | data                         | verification, calib.                                   | $(\theta, \theta')$ symmetry                                                                                                      |                                                                                                                                                                                                                       |
| Coupling                | $\theta(w), (\theta_1(w), \theta_2(w))$     | rng-share process            | unbias, var. reduction                                 |                                                                                                                                   | [Maximal Couplings of the Metropolis–Hastings Algorithm](http://proceedings.mlr.press/v130/wang21d/wang21d.pdf)                                                                                                       |
| Splitting               | $\theta(w), (\theta_1(w), \theta_2(w)?)$    | ?                            |                                                        |                                                                                                                                   |                                                                                                                                                                                                                       |
| Copula                  | $x_a, (x_a, x_b)$                           | param. with dependence       | dependence inf, calib.                                 | Uniform marginal                                                                                                                  |                                                                                                                                                                                                                       |
| Info. bottleneck        | $x, (x,\tilde{x})$                          | encode of the source         | $p(\tilde{x}/x)$ optimal assignment                    | self-consistent eq. for $X \rightarrow \tilde{X},\tilde{X} \rightarrow Y$ coding <br> marginal for p$(y/\tilde{x}), p(\tilde{x})$ | [Tishby2000](https://arxiv.org/abs/physics/0004057)(Thm.4)                                                                                                                                                            |
| EM                      | $\theta, (\theta, z)$                       | latent variable              | sampling is easier for $p(\theta/z)$ than $p(\theta )$ | $p(\theta) =\int p(\theta/z)dz$                                                                                                   |                                                                                                                                                                                                                       |
| Imputation              | $\theta, (\theta, z)$                       | data augmentation            |                                                        |                                                                                                                                   |                                                                                                                                                                                                                       |
| CVaR MDP1               | $X, (X , C)$                                | running cost state           |                                                        |                                                                                                                                   | Bäuerle(2011), Huang(2016), Miller(2017), Chow(2018), Backhoff-Veraguas(2020) from [Min20](http://www.mskyt.net/wp-content/uploads/2020/11/cvar-exec.pdf)                                                             |
| CVaR MDP2               | $X, (X , Q)$                                | risk aversion quantile state | CVaR dual                                              |                                                                                                                                   | Pflug(2016), [Chow et al. (2015)](https://papers.nips.cc/paper/2015/file/64223ccf70bbb65a3a4aceac37e21016-Paper.pdf), Chapman(2018), Li(2020), [Min20](http://www.mskyt.net/wp-content/uploads/2020/11/cvar-exec.pdf) |
| Lagrangian dual         | $x, (x,\lambda)$                            | coeff. dual variable         | dual's convex strc.                                    | dual ineq.  infsup>=supinf                                                                                                        |                                                                                                                                                                                                                       |
| Aug. Lagrangian or ADMM | $x, (x, \lambda)$ or $x,z,  (x,z, \lambda)$ | coeff. dual variable         | convex strc.,better convergence                        | dual ineq.                                                                                                                        | [A generalized risk budgeting approach to portfolio construction](http://www.columbia.edu/~mh2078/A_generalized_risk_budgeting_approach.pdf)                                                                          |
| Stopping time           | $X, (\alpha,X_\alpha)$                      | stopping time                | Markovian strc.                                        |                                                                                                                                   | Chung, Intro to prob. Ch.9                                                                                                                                                                                            |
| HMC                     | $q, (p,q)$                                  | momentum in phase space      | Vector flow structure                                  | Hamiltonian symplectic vol.                                                                                                       |                                                                                                                                                                                                                       |
| Augmented Neural ODE    | $x, (x_1, x_2)$?                            | feature                      | flexible feature mapping                               |                                                                                                                                   | [Augmented Neural ODEs](https://arxiv.org/abs/1904.01681)                                                                                                                                                             |
| Augmented preference    | ?                                           | augmented preference         | infer preference w.o. util. ftn.                       | partial order                                                                                                                     | Boyd, Convex Optimization Ch.6                                                                                                                                                                                        |

2. Iterate augmentation/projection: Origin $X_t$ $\xrightarrow{\text{Lift}\; f}$  Augmented ($X_t, f(X_t)$)  $\xrightarrow{\text{Project/Update}\; f^{-1}}$  Origin $X_{t+1}$
- similar to Source ($X_t, P$) $\xrightarrow{\text{Compressor}\; f}$  Augmented ($X_t, f(X_t)$)  $\xrightarrow{\text{Decompressor}\; P(X_{t+1}|f(X_t))}$  Receiver $X_{t+1}$
- used for efficient update (=calibration) or verification via model boostrap
 
| algorithm               | Update, <br>Calibration target                                                                                            | Lift f                        | Projection $f^{-1}$               |
| ----------------------- | ------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | --------------------------------- |
| SBC                     | $\theta_t \xrightarrow{(\theta_t, y_t)} \theta_{t+1}$,<br>joint dist.simulator                                            | $p(\theta/y)$ data simulator  | $p(y/\theta)$ posterior simulator |
| Copula                  | $x^a_t \xrightarrow{(x^a_t, x^b_t)} x^a_{t+1},x^b_t \xrightarrow{(x^a_t, x^b_t)} x^b_{t+1}$,<br>copula ftn. $C(x_a,x_b)$  |                               |                                   |
| Info. bottleneck        | $x_t \xrightarrow{(x_t, \tilde{x}_t)} x_{t+1}$,<br> maximum rate $p^{*}(\tilde{x}/x)$                                     |                               |                                   |
| EM                      | $\theta_t/Y \xrightarrow{(\theta_t, z_t)/Y} \theta_{t+1}/Y$,<br> $p(\theta/Y)$                                            | $p(\theta / z)$               |                                   |
| Imputation              | $\theta_t \xrightarrow{(\theta_t, z_t)} \theta_{t+1}$,<br> $p(\theta)$                                                    | $Z_{t+1} \sim P (Z / Y, θ_t)$ | $θ_{t+1} \sim P (θ / Y, Z_{t+1})$ |
| CVaR MDP1               | $X_t \xrightarrow{(X_t, C_t)} X_{t+1}$,<br>optimum                                                                        |                               |                                   |
| CVaR MDP2               | $X_t \xrightarrow{(X_t, Q_t)} X_{t+1}$,<br>optimum                                                                        |                               |                                   |
| Lagrangian dual         | $x_t \xrightarrow{(x_t,\lambda_t)} x_{t+1}$,<br>optimum                                                                   | $f(x) +\lambda g(x)$          |                                   |
| Aug. Lagrangian or ADMM | $x_t \xrightarrow{(x_t, \lambda_t)} x_{t+1}$ or $x_t,z_t \xrightarrow{(x_t,z_t, \lambda_t)} x_{t+1}, z_{t+1}$,<br>optimum |                               |                                   |
| HMC                     | $q_t \xrightarrow{(p_t,q_t)} q_{t+1}$,<br>typical set                                                                     | $q \rightarrow \pi^{-1}(q)$   |                                   |
| Augmented Neural ODE    | $x^a_t \xrightarrow{(x^a_t, x^b_2)} x^a_{t+1}$ (Incorrect),<br> ?                                                         |                               |                                   |

Q. 
1. Do you know why Markov chain direction is $\tilde{X} ← X ← Y$ from Information bottleneck theorem (footnote 3 from the paper)?
2. What do you think is the difference between splitting and coupling? Do you agree they have similar flavor; the only difference being the direction of the bifurcation --= vs =--? 

Quotes from papers
- combining the augmented Lagrangian approach with MCMC sampling to generate a point in the proximity of the global optimum of the GRB problem. sample points with a higher objective function value and simultaneously drive the sample path in the direction of the feasible region using the augmented Lagrangian terms. -  [A generalized risk budgeting approach to portfolio construction](http://www.columbia.edu/~mh2078/A_generalized_risk_budgeting_approach.pdf) 
 - by introducing an extra state variable, an optimal policy can be sufficiently characterized as a Markov process defined on this augmented state space. - Min's [paper](http://www.mskyt.net/wp-content/uploads/2020/11/cvar-exec.pdf)
 - augment the known preferences (6.22) with the inequality u(ak) ≤ u(al). If augmented set of preferences is infeasible, it means that any concave nondecreasing utility function that is consistent with the original given consumer preference data must also satisfy u(ak) > u(al); conclude that basket k is preferred to basket l, without knowing the underlying utility function.  - Boyd's convex opt. textbook p.341
	 - It seems the role of "Augmented preference"  is feasibility certificate, but its role is tricky to understand. Especially does 'augment' mean sampling in this context?
